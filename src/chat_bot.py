"""
GPT-4 Integration Tutorial

This file serves as a tutorial for Brunel University students, teaching them how to integrate GPT-4 into their projects using OpenAI and Streamlit.

Installation
~~~~~~~~~~~~

To run this file, please follow these steps:

1. Ensure that you have obtained access to the OpenAI GPT-4 API. You will need to provide your API key to authenticate with OpenAI.

2. Install the required libraries by executing the following command:

    pip install -r requirements.txt

   This command will install the necessary dependencies listed in the requirements.txt file.

3. Rename the secrets_temp.toml file in the .streamlit directory to secrets.toml. This file is used to securely store your OpenAI API key.

   Note: Do not share or commit your API key to a public repository.

4. Open the secrets.toml file in a text editor and replace the placeholder value with your actual OpenAI API key.

Usage
~~~~~

To run the tutorial, execute the following command:

    streamlit run gpt4_integration.py

This command will start the Streamlit application.

Next, open your web browser and visit the provided local URL (usually http://localhost:8501/) to access the Streamlit app.

You will be presented with a user interface where you can input desired text prompts and interact with GPT-4.

Follow the instructions and experiment with different prompts to see GPT-4's responses.

Enjoy exploring the capabilities of GPT-4 through this integration tutorial!

Please Note
~~~~~~~~~~~

- As GPT-4 is a language model developed by OpenAI, it is imperative to ensure compliance with OpenAI's usage policies and any restrictions placed on the usage of the GPT-4 model.

- This tutorial assumes that you have prior knowledge of Python, OpenAI, and Streamlit.

- Remember to modify the code to suit your specific needs and projects.

Generated by OpenAI's Assistant :)
"""


import openai
import streamlit as st

from utilities import pdf_to_text, split_text


def run_bot():
    """
    Runs the chatbot, maintaining a conversation with the user
    through the Streamlit interface.

    This function retrieves user input with Streamlit's text_input
    function and sends the input to the GPT-4 model. The model's
    response is then displayed on the interface.

    Args:
        None.

    Returns:
        None.
    """

    with st.sidebar:
        st.title('Brunel Chatbot')

        api_key = st.text_input('Enter OpenAI API Key', type='password')
        if api_key:
            st.success('API key loaded!', icon='âœ…')
            openai.api_key = api_key

        st.subheader('Prompt type')
        prompt_type = st.selectbox('Select Prompt type', ['text', 'image', 'pdf'])

        if prompt_type == 'text':
            st.subheader('GPT Version')
            gpt_version = st.selectbox('Select GPT Version', ['gpt-3.5-turbo', 'gpt-4', 'gpt-4-1106-preview', 'gpt-4-vision-preview'])
            token_number = st.number_input('Insert Token Max Size', min_value=1, max_value=2048, value=1, step=1)
            temperature = st.number_input('Insert Temperature', min_value=0.1, max_value=1.0, value=0.1, step=0.1)
            st.write('You selected:', gpt_version)
            st.write('Token max size is:', token_number)
            st.write('Temperature is:', temperature)

        elif prompt_type == 'pdf':
            st.subheader('Configurations')
            gpt_version = st.selectbox('Select the Engine', ['davinci'])
            token_number = st.number_input('Insert Token Max Size', min_value=1, max_value=2048, value=1, step=1)
            temperature = st.number_input('Insert Temperature', min_value=0.1, max_value=1.0, value=0.1, step=0.1)
            st.write('You selected:', gpt_version)
            st.write('Token max size is:', token_number)
            st.write('Temperature is:', temperature)

        elif prompt_type == 'image':
            st.subheader('Configurations')
            number_of_results = st.number_input('Insert number of results', min_value=1, max_value=5, value=1, step=1)

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt_type == 'text':
        if prompt := st.chat_input("What is up?"):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                for response in openai.ChatCompletion.create(
                        model=gpt_version,
                        messages=[{"role": m["role"], "content": m["content"]}
                                  for m in st.session_state.messages],
                        stream=True,
                        temperature=temperature,
                        max_tokens=token_number):
                    full_response += response.choices[0].delta.get("content", "")
                    message_placeholder.markdown(full_response + " ")
                message_placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})

    elif prompt_type == 'image':
        if prompt := st.chat_input("What is up?"):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = f""
                response = openai.Image.create(
                    n=number_of_results,
                    prompt=prompt,
                    size='256x256'
                )
                for result in response['data']:
                    full_response += f"![Image]({result['url']}) \n"
                    message_placeholder.markdown(full_response + "\n")
                message_placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})

    elif prompt_type == 'pdf':
        uploaded_pdf = st.file_uploader("Upload a PDF file", type=["pdf"])
        if uploaded_pdf:
            st.write("PDF file uploaded successfully!")
            extracted_pdf = pdf_to_text(uploaded_pdf, st)
            if prompt := st.chat_input("What is up?"):

                st.session_state.messages.append({"role": "user", "content": f"{prompt} \n {extracted_pdf}"})

                with st.chat_message("user"):
                    st.markdown(f"{prompt} \n ...")
                with st.chat_message("assistant"):
                    message_placeholder = st.empty()
                    full_response = ""

                    text_segments = split_text(extracted_pdf, max_tokens=token_number)

                    for segment in text_segments:
                        response = openai.Completion.create(
                            engine=gpt_version,
                            prompt=f"{prompt} \n{segment}",
                            max_tokens=token_number,
                            temperature=temperature,
                        )
                        full_response += response.choices[0].text
                        message_placeholder.markdown(full_response + " ")
                    message_placeholder.markdown(full_response)
                st.session_state.messages.append({"role": "assistant", "content": full_response})


if __name__ == "__main__":
    run_bot()
